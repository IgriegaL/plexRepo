version: "3.9"

# Servicios de Seguridad
# Uso: docker-compose -f docker-compose.yml -f docker-compose.security.yml up -d

services:
  # ------------------- Autenticaci√≥n 2FA -------------------
  authelia:
    image: authelia/authelia:latest
    container_name: authelia
    restart: unless-stopped
    environment:
      - TZ=${TZ}
      - AUTHELIA_JWT_SECRET_FILE=/run/secrets/authelia_jwt_secret
      - AUTHELIA_SESSION_SECRET_FILE=/run/secrets/authelia_session_secret
      - AUTHELIA_STORAGE_ENCRYPTION_KEY_FILE=/run/secrets/authelia_storage_key
    volumes:
      - ./authelia:/config
    ports:
      - "9091:9091"
    secrets:
      - authelia_jwt_secret
      - authelia_session_secret
      - authelia_storage_key
    networks:
      - frontend
      - backend
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9091/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.authelia.rule=Host(`auth.${DOMAIN}`)"
      - "traefik.http.routers.authelia.entrypoints=websecure"
      - "traefik.http.routers.authelia.tls.certresolver=letsencrypt"
      - "traefik.http.middlewares.authelia.forwardauth.address=http://authelia:9091/api/verify?rd=https://auth.${DOMAIN}"
      - "traefik.http.middlewares.authelia.forwardauth.trustForwardHeader=true"
      - "traefik.http.middlewares.authelia.forwardauth.authResponseHeaders=Remote-User,Remote-Groups,Remote-Name,Remote-Email"

  # ------------------- Fail2ban -------------------
  fail2ban:
    image: crazymax/fail2ban:latest
    container_name: fail2ban
    restart: unless-stopped
    network_mode: host
    cap_add:
      - NET_ADMIN
      - NET_RAW
    volumes:
      - ./fail2ban:/data
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    environment:
      - TZ=${TZ}
      - F2B_LOG_LEVEL=INFO
      - F2B_DB_PURGE_AGE=30d
      - F2B_MAX_RETRY=5
      - F2B_DEST_EMAIL=${SECURITY_ALERT_EMAIL}
      - F2B_SENDER=fail2ban@${DOMAIN}
      - F2B_ACTION=%(action_mwl)s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ------------------- Antivirus -------------------
  clamav:
    image: clamav/clamav:latest
    container_name: clamav
    restart: unless-stopped
    volumes:
      - ${DOWNLOADS_VOLUME}:/scan:ro
      - ./clamav/signatures:/var/lib/clamav
      - ./clamav/quarantine:/quarantine
    environment:
      - TZ=${TZ}
      - CLAMAV_NO_FRESHCLAMD=false
      - CLAMAV_NO_CLAMD=false
    mem_limit: 2g
    cpus: 1
    networks:
      - backend
    healthcheck:
      test: ["CMD", "clamdscan", "--ping", "1"]
      interval: 60s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ------------------- WAF (Web Application Firewall) -------------------
  modsecurity:
    image: owasp/modsecurity-crs:nginx
    container_name: modsecurity-waf
    restart: unless-stopped
    environment:
      - PARANOIA=2
      - ANOMALY_INBOUND=5
      - ANOMALY_OUTBOUND=4
      - BLOCKING_PARANOIA=2
      - TZ=${TZ}
    ports:
      - "8888:80"
    volumes:
      - ./modsecurity/rules:/etc/modsecurity.d/owasp-crs/rules
      - ./modsecurity/logs:/var/log/modsecurity
    networks:
      - frontend
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ------------------- Logs Centralizados -------------------
  # Loki and Promtail were removed from the security compose by repository maintainers
  # because some deployments do not use centralized logging and it caused mount/port conflicts.
  # To re-enable Loki/Promtail, add the `loki` and `promtail` service blocks back and ensure
  # the files `./loki/loki-config.yml` and `./promtail/promtail-config.yml` exist and are files (not directories).

  # ------------------- Escaneo de Vulnerabilidades -------------------
  trivy:
    image: aquasec/trivy:latest
    container_name: trivy
    restart: "no"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./trivy/cache:/root/.cache/
      - ./trivy/reports:/reports
    command: ["server", "--listen", "0.0.0.0:8080"]
    ports:
      - "8082:8080"
    networks:
      - monitoring
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ------------------- Intrusion Detection -------------------
  crowdsec:
    image: crowdsecurity/crowdsec:latest
    container_name: crowdsec
    restart: unless-stopped
    environment:
      - COLLECTIONS=crowdsecurity/nginx crowdsecurity/http-cve
      - TZ=${TZ}
    volumes:
      - ./crowdsec/config:/etc/crowdsec
      - ./crowdsec/data:/var/lib/crowdsec/data
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    networks:
      - frontend
      - monitoring
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

# ------------------- Secrets -------------------
secrets:
  authelia_jwt_secret:
    file: ./secrets/authelia_jwt_secret.txt
  authelia_session_secret:
    file: ./secrets/authelia_session_secret.txt
  authelia_storage_key:
    file: ./secrets/authelia_storage_key.txt

# ------------------- Redes Segmentadas -------------------
networks:
  frontend:
    driver: bridge
    name: frontend
    ipam:
      config:
        - subnet: 172.20.0.0/16
  
  backend:
    driver: bridge
    name: backend
    internal: true  # Sin acceso directo a internet
    ipam:
      config:
        - subnet: 172.21.0.0/16
  
  vpn:
    driver: bridge
    name: vpn
    ipam:
      config:
        - subnet: 172.22.0.0/16
